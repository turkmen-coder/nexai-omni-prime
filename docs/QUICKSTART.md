# ğŸš€ NEXA Projesi - HÄ±zlÄ± BaÅŸlangÄ±Ã§ Rehberi (Ollama)

## 3 AdÄ±mda Yerel Kurulum

### 1ï¸âƒ£ Ollama'yÄ± Kurun ve BaÅŸlatÄ±n

```bash
# Linux/Mac
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# https://ollama.com/download/windows adresinden indirin

# Model indirin
ollama pull llama3.2

# Servisi baÅŸlatÄ±n
ollama serve
```

### 2ï¸âƒ£ Projeyi Kurun

```bash
git clone https://github.com/turkmen-coder/NEXA-.git
cd NEXA-
npm install
```

### 3ï¸âƒ£ Ã‡alÄ±ÅŸtÄ±rÄ±n

```bash
npm run dev
```

**TarayÄ±cÄ±da aÃ§Ä±n:** http://localhost:5173

---

## âœ… BaÅŸarÄ±lÄ± Kurulum KontrolÃ¼

Proje baÅŸlatÄ±ldÄ±ktan sonra:

1. TarayÄ±cÄ±da `http://localhost:5173` aÃ§Ä±n
2. KayÄ±t olun veya giriÅŸ yapÄ±n
3. AI ile sohbet edin
4. TarayÄ±cÄ± console'unda ÅŸunu gÃ¶rmelisiniz:
   ```
   AI Response from: ollama
   ```

---

## ğŸ”§ Sorun mu YaÅŸÄ±yorsunuz?

### Ollama BaÄŸlantÄ± HatasÄ±

```bash
# Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun
ollama serve

# BaÅŸka bir terminalde test edin
curl http://localhost:11434/api/tags
```

### Model BulunamadÄ±

```bash
# Modeli indirin
ollama pull llama3.2

# YÃ¼klÃ¼ modelleri kontrol edin
ollama list
```

---

## ğŸ“– Daha Fazla Bilgi

- **DetaylÄ± Kurulum:** [OLLAMA_SETUP.md](./OLLAMA_SETUP.md)
- **Entegrasyon DetaylarÄ±:** [OLLAMA_INTEGRATION_SUMMARY.md](./OLLAMA_INTEGRATION_SUMMARY.md)
- **Proje DokÃ¼mantasyonu:** [README.md](./README.md)

---

**ğŸ‰ Hepsi bu kadar! ArtÄ±k NEXA projeniz yerel olarak Ã§alÄ±ÅŸÄ±yor!**
