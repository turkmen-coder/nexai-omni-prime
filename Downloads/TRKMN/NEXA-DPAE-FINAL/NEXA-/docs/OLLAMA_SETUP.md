# NEXA Projesi - Ollama Yerel Kurulum Rehberi

Bu rehber, NEXA projesini Ollama ile yerel olarak Ã§alÄ±ÅŸtÄ±rmak iÃ§in gerekli adÄ±mlarÄ± iÃ§erir.

## ğŸ“‹ Gereksinimler

- Node.js 18+ 
- pnpm veya npm
- Ollama (yerel AI modeli Ã§alÄ±ÅŸtÄ±rmak iÃ§in)

## ğŸš€ Kurulum AdÄ±mlarÄ±

### 1. Ollama Kurulumu

#### Linux/Mac:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### Windows:
[Ollama Windows Ä°ndirme SayfasÄ±](https://ollama.com/download/windows) Ã¼zerinden indirip kurun.

### 2. Ollama Modelini Ä°ndirin ve EÄŸitin (NEXAI PERSONA)

Proje artÄ±k Ã¶zel bir **NEXAI** modeli kullanmaktadÄ±r. Bu modeli oluÅŸturmak iÃ§in ÅŸu adÄ±mlarÄ± izleyin:

1. Ã–nce temel modeli indirin:
```bash
ollama pull llama3.2
```

2. Proje dizinindeki `nexai.Modelfile` dosyasÄ±nÄ± kullanarak yerel NEXAI modelinizi oluÅŸturun:
```bash
ollama create nexai -f nexai.Modelfile
```

Bu iÅŸlemden sonra Ollama, NEXAI OMNI-PRIME kiÅŸiliÄŸiyle cevap verecek ÅŸekilde yapÄ±landÄ±rÄ±lmÄ±ÅŸ olacaktÄ±r.

**Alternatif Modeller (Gerekirse):**
```bash
# Daha kÃ¼Ã§Ã¼k model (hÄ±zlÄ±)
ollama pull llama3.2:1b

# Daha bÃ¼yÃ¼k model (daha iyi sonuÃ§lar)
ollama pull llama3.2:3b

# TÃ¼rkÃ§e iÃ§in optimize edilmiÅŸ
ollama pull gemma2:2b

# DiÄŸer popÃ¼ler modeller
ollama pull mistral
ollama pull phi3
```

### 3. Ollama Servisini BaÅŸlatÄ±n

```bash
ollama serve
```

Ollama varsayÄ±lan olarak `http://localhost:11434` adresinde Ã§alÄ±ÅŸÄ±r.

### 4. Proje BaÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± Kurun

```bash
cd NEXA-
pnpm install
# veya
npm install
```

### 5. GeliÅŸtirme Sunucusunu BaÅŸlatÄ±n

```bash
pnpm dev
# veya
npm run dev
```

Uygulama `http://localhost:5173` adresinde Ã§alÄ±ÅŸacaktÄ±r.

## âš™ï¸ YapÄ±landÄ±rma

### Model DeÄŸiÅŸtirme

`src/index.tsx` dosyasÄ±nda `AI_CONFIG.OLLAMA.models` bÃ¶lÃ¼mÃ¼nÃ¼ dÃ¼zenleyin:

```typescript
OLLAMA: {
  baseUrl: 'http://localhost:11434/api',
  models: {
    pro: 'llama3.2',        // BurasÄ± deÄŸiÅŸtirilebilir
    flash: 'llama3.2',      // BurasÄ± deÄŸiÅŸtirilebilir
    thinking: 'llama3.2'    // BurasÄ± deÄŸiÅŸtirilebilir
  },
  temperature: { logic: 0.1, creative: 0.8, analysis: 0.3 }
}
```

### Ollama Portunu DeÄŸiÅŸtirme

EÄŸer Ollama farklÄ± bir portta Ã§alÄ±ÅŸÄ±yorsa:

```typescript
OLLAMA: {
  baseUrl: 'http://localhost:YENI_PORT/api',
  // ...
}
```

## ğŸ”„ AI Ã–ncelik SÄ±rasÄ±

Proje ÅŸu sÄ±rayla AI servislerini dener:

1. **Ollama** (Yerel) - Ã–ncelikli
2. **Google Gemini** (API Key gerekli) - Yedek
3. **OpenRouter** (API Key gerekli) - Son yedek

Ollama Ã§alÄ±ÅŸmazsa otomatik olarak diÄŸer servislere geÃ§er.

## ğŸ§ª Test Etme

### Ollama Servisini Test Edin:

```bash
curl http://localhost:11434/api/tags
```

Ã‡Ä±ktÄ±, yÃ¼klÃ¼ modelleri gÃ¶stermelidir.

### Model ile Chat Testi:

```bash
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "Merhaba, nasÄ±lsÄ±n?"
    }
  ],
  "stream": false
}'
```

## ğŸ“Š Performans Ä°puÃ§larÄ±

### GPU KullanÄ±mÄ±
Ollama otomatik olarak NVIDIA GPU'yu algÄ±lar ve kullanÄ±r. CPU'da da Ã§alÄ±ÅŸÄ±r ancak daha yavaÅŸtÄ±r.

### Bellek KullanÄ±mÄ±
- `llama3.2:1b` â†’ ~1GB RAM
- `llama3.2` â†’ ~4GB RAM
- `llama3.2:3b` â†’ ~6GB RAM

### HÄ±z Optimizasyonu
```bash
# Daha hÄ±zlÄ± yanÄ±t iÃ§in context boyutunu azaltÄ±n
ollama run llama3.2 --num-ctx 2048
```

## ğŸ› Sorun Giderme

### Ollama BaÄŸlantÄ± HatasÄ±

**Hata:** `Failed to connect to Ollama`

**Ã‡Ã¶zÃ¼m:**
1. Ollama servisinin Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun: `ollama serve`
2. Port'un doÄŸru olduÄŸunu kontrol edin
3. Firewall ayarlarÄ±nÄ± kontrol edin

### Model BulunamadÄ±

**Hata:** `model 'llama3.2' not found`

**Ã‡Ã¶zÃ¼m:**
```bash
ollama pull llama3.2
```

### YavaÅŸ YanÄ±t

**Ã‡Ã¶zÃ¼m:**
- Daha kÃ¼Ã§Ã¼k bir model kullanÄ±n (`llama3.2:1b`)
- GPU kullanÄ±n
- `num_predict` deÄŸerini azaltÄ±n

## ğŸŒ Ãœretim OrtamÄ±

Ãœretim ortamÄ±nda Ollama kullanmak iÃ§in:

1. Ollama'yÄ± sunucuya kurun
2. `AI_CONFIG.OLLAMA.baseUrl` deÄŸerini sunucu adresine gÃ¼ncelleyin
3. GÃ¼venlik iÃ§in API authentication ekleyin

```typescript
OLLAMA: {
  baseUrl: 'https://your-ollama-server.com/api',
  // ...
}
```

## ğŸ“š Ek Kaynaklar

- [Ollama Resmi DokÃ¼mantasyonu](https://github.com/ollama/ollama)
- [Ollama Model KÃ¼tÃ¼phanesi](https://ollama.com/library)
- [NEXA Proje DokÃ¼mantasyonu](./README.md)

## ğŸ¤ KatkÄ±da Bulunma

Sorun bildirmek veya Ã¶neride bulunmak iÃ§in GitHub Issues kullanÄ±n.

---

**Not:** Bu proje artÄ±k tamamen yerel olarak, internet baÄŸlantÄ±sÄ± olmadan Ã§alÄ±ÅŸabilir! ğŸ‰
