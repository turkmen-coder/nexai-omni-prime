# NEXA Projesi - Ollama Entegrasyon Ã–zeti

## ğŸ¯ YapÄ±lan DeÄŸiÅŸiklikler

NEXA projesi baÅŸarÄ±yla **Ollama** ile yerel Ã§alÄ±ÅŸacak ÅŸekilde yapÄ±landÄ±rÄ±ldÄ±. ArtÄ±k internet baÄŸlantÄ±sÄ± olmadan tamamen yerel AI modelleri kullanabilirsiniz!

---

## ğŸ“ Kod DeÄŸiÅŸiklikleri

### 1. `src/index.tsx` - AI Configuration GÃ¼ncellemesi

**Eklenen:** Ollama yapÄ±landÄ±rmasÄ±

```typescript
OLLAMA: {
  baseUrl: 'http://localhost:11434/api',
  models: {
    pro: 'llama3.2',
    flash: 'llama3.2',
    thinking: 'llama3.2'
  },
  temperature: { logic: 0.1, creative: 0.8, analysis: 0.3 }
}
```

### 2. `src/index.tsx` - callOllama Fonksiyonu Eklendi

Yeni fonksiyon Ollama API ile iletiÅŸim kurar:

```typescript
async function callOllama(model: string, messages: any[], temperature: number, options: any = {})
```

**Ã–zellikler:**
- Ollama chat API'sine uyumlu mesaj formatÄ±
- Stream desteÄŸi (ÅŸu an kapalÄ±, aÃ§Ä±labilir)
- Token kullanÄ±m istatistikleri
- Hata yÃ¶netimi

### 3. `src/index.tsx` - callAI Fonksiyonu GÃ¼ncellendi

**Yeni AI Ã–ncelik SÄ±rasÄ±:**

1. **Ollama** (Yerel) - Ã–ncelikli âœ…
2. **Google Gemini** (Cloud) - Fallback 1
3. **OpenRouter** (Cloud) - Fallback 2

```typescript
async function callAI(env: Bindings, messages: any[], temperature: number = 0.3, options: any = {}) {
  // Try Ollama first (local)
  const ollamaResult = await callOllama(model, messages, temperature, options)
  if (ollamaResult?.content) {
    return { content: ollamaResult.content, model: `ollama/${ollamaResult.model}`, source: 'ollama' }
  }
  
  // Fallback to Gemini...
  // Fallback to OpenRouter...
}
```

---

## ğŸ“š DokÃ¼mantasyon GÃ¼ncellemeleri

### 1. `OLLAMA_SETUP.md` (YENÄ°)

DetaylÄ± Ollama kurulum ve yapÄ±landÄ±rma rehberi:

- âœ… Linux/Mac/Windows kurulum talimatlarÄ±
- âœ… Model indirme ve yÃ¶netimi
- âœ… Proje yapÄ±landÄ±rmasÄ±
- âœ… Performans optimizasyonu
- âœ… Sorun giderme
- âœ… Alternatif model Ã¶nerileri

### 2. `README.md` GÃ¼ncellemeleri

**DeÄŸiÅŸiklikler:**

- âœ… AI Engine badge'i gÃ¼ncellendi: "Ollama (Local)"
- âœ… AI Architecture diyagramÄ± gÃ¼ncellendi
- âœ… Yerel kurulum bÃ¶lÃ¼mÃ¼ eklendi (Ollama ile)
- âœ… Cloud kurulum ayrÄ± bÃ¶lÃ¼m olarak dÃ¼zenlendi
- âœ… Environment deÄŸiÅŸkenleri artÄ±k opsiyonel

---

## ğŸš€ KullanÄ±m TalimatlarÄ±

### HÄ±zlÄ± BaÅŸlangÄ±Ã§

```bash
# 1. Ollama'yÄ± kurun
curl -fsSL https://ollama.com/install.sh | sh

# 2. Model indirin
ollama pull llama3.2

# 3. Ollama servisini baÅŸlatÄ±n
ollama serve

# 4. Projeyi Ã§alÄ±ÅŸtÄ±rÄ±n
cd NEXA-
npm install
npm run dev
```

### Model DeÄŸiÅŸtirme

`src/index.tsx` dosyasÄ±nda istediÄŸiniz modeli seÃ§in:

```typescript
OLLAMA: {
  models: {
    pro: 'llama3.2',        // DeÄŸiÅŸtirilebilir
    flash: 'mistral',       // DeÄŸiÅŸtirilebilir
    thinking: 'gemma2:2b'   // DeÄŸiÅŸtirilebilir
  }
}
```

**PopÃ¼ler Alternatifler:**
- `llama3.2` (VarsayÄ±lan, dengeli)
- `llama3.2:1b` (HÄ±zlÄ±, dÃ¼ÅŸÃ¼k kaynak)
- `llama3.2:3b` (YÃ¼ksek kalite)
- `mistral` (HÄ±zlÄ± ve akÄ±llÄ±)
- `gemma2:2b` (TÃ¼rkÃ§e iÃ§in iyi)
- `phi3` (KÃ¼Ã§Ã¼k ve hÄ±zlÄ±)

---

## ğŸ”„ Fallback MekanizmasÄ±

Ollama Ã§alÄ±ÅŸmazsa otomatik olarak cloud servislere geÃ§er:

```
Ollama (Local) âŒ
    â†“
Google Gemini (Cloud) âœ…
    â†“
OpenRouter (Cloud) âœ…
```

Bu sayede sistem her zaman Ã§alÄ±ÅŸÄ±r durumda kalÄ±r!

---

## ğŸ“Š Avantajlar

### Ollama KullanmanÄ±n FaydalarÄ±:

| Ã–zellik | Ollama (Yerel) | Cloud API |
|---------|----------------|-----------|
| **Maliyet** | âœ… Ãœcretsiz | âŒ API Ã¼cretli |
| **Gizlilik** | âœ… Veriler yerel | âŒ Sunucuya gÃ¶nderilir |
| **Ä°nternet** | âœ… Gerekmez | âŒ Gerekli |
| **HÄ±z** | âœ… DÃ¼ÅŸÃ¼k latency | âš ï¸ Network'e baÄŸlÄ± |
| **Kontrol** | âœ… Tam kontrol | âŒ SÄ±nÄ±rlÄ± |

---

## ğŸ§ª Test Etme

### Ollama Servisini Test Edin:

```bash
# YÃ¼klÃ¼ modelleri listele
curl http://localhost:11434/api/tags

# Chat testi
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [{"role": "user", "content": "Merhaba!"}],
  "stream": false
}'
```

### Proje Testi:

1. `npm run dev` ile projeyi baÅŸlatÄ±n
2. TarayÄ±cÄ±da `http://localhost:5173` aÃ§Ä±n
3. KayÄ±t olun veya giriÅŸ yapÄ±n
4. AI ile sohbet edin
5. Console'da "ollama" source'u gÃ¶rmelisiniz

---

## ğŸ› Bilinen Sorunlar ve Ã‡Ã¶zÃ¼mler

### Sorun 1: "Failed to connect to Ollama"

**Ã‡Ã¶zÃ¼m:**
```bash
# Ollama servisini baÅŸlatÄ±n
ollama serve
```

### Sorun 2: "Model not found"

**Ã‡Ã¶zÃ¼m:**
```bash
# Modeli indirin
ollama pull llama3.2
```

### Sorun 3: YavaÅŸ yanÄ±t

**Ã‡Ã¶zÃ¼m:**
- Daha kÃ¼Ã§Ã¼k model kullanÄ±n: `ollama pull llama3.2:1b`
- GPU kullanÄ±n (otomatik algÄ±lanÄ±r)
- Context boyutunu azaltÄ±n

---

## ğŸ“ˆ Performans Ä°puÃ§larÄ±

### Bellek KullanÄ±mÄ±

- `llama3.2:1b` â†’ ~1GB RAM (hÄ±zlÄ±)
- `llama3.2` â†’ ~4GB RAM (dengeli)
- `llama3.2:3b` â†’ ~6GB RAM (yÃ¼ksek kalite)

### GPU DesteÄŸi

Ollama otomatik olarak NVIDIA GPU'yu algÄ±lar ve kullanÄ±r:

```bash
# GPU kullanÄ±mÄ±nÄ± kontrol edin
nvidia-smi
```

### HÄ±z Optimizasyonu

```bash
# Context boyutunu azaltarak hÄ±zlandÄ±rÄ±n
ollama run llama3.2 --num-ctx 2048
```

---

## ğŸ” GÃ¼venlik NotlarÄ±

- Ollama yerel olarak Ã§alÄ±ÅŸtÄ±ÄŸÄ± iÃ§in verileriniz hiÃ§bir sunucuya gÃ¶nderilmez
- API key'lere ihtiyaÃ§ yoktur
- Tamamen offline Ã§alÄ±ÅŸabilir
- Kendi bilgisayarÄ±nÄ±zda tam kontrol

---

## ğŸ“ Ek Kaynaklar

- [Ollama Resmi DokÃ¼mantasyonu](https://github.com/ollama/ollama)
- [Ollama Model KÃ¼tÃ¼phanesi](https://ollama.com/library)
- [NEXA Proje README](./README.md)
- [Ollama Kurulum Rehberi](./OLLAMA_SETUP.md)

---

## ğŸ¤ KatkÄ±da Bulunma

Bu entegrasyon hakkÄ±nda geri bildirim veya Ã¶nerileriniz iÃ§in GitHub Issues kullanÄ±n.

---

## âœ… Kontrol Listesi

Entegrasyon tamamlandÄ±:

- [x] Ollama API entegrasyonu eklendi
- [x] callOllama fonksiyonu yazÄ±ldÄ±
- [x] AI Ã¶ncelik sÄ±rasÄ± gÃ¼ncellendi
- [x] Fallback mekanizmasÄ± korundu
- [x] README gÃ¼ncellendi
- [x] OLLAMA_SETUP.md oluÅŸturuldu
- [x] Proje baÅŸarÄ±yla build edildi
- [x] DokÃ¼mantasyon tamamlandÄ±

---

**ğŸ‰ Tebrikler! NEXA projeniz artÄ±k Ollama ile yerel olarak Ã§alÄ±ÅŸÄ±yor!**

Herhangi bir sorun yaÅŸarsanÄ±z `OLLAMA_SETUP.md` dosyasÄ±na bakÄ±n veya GitHub Issues'da sorun bildirin.
